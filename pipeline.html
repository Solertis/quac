

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>3. Preprocessing data &mdash; QUAC documentation for commit 6b395f2</title>
    
    <link rel="stylesheet" href="_static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '',
        VERSION:     '0.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="top" title="QUAC documentation for commit 6b395f2" href="index.html" />
    <link rel="next" title="4. Map-Reduce with QUACreduce" href="map_reduce.html" />
    <link rel="prev" title="2. Collecting data" href="collecting.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="map_reduce.html" title="4. Map-Reduce with QUACreduce"
             accesskey="N">next</a></li>
        <li class="right" >
          <a href="collecting.html" title="2. Collecting data"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">QUAC documentation for commit 6b395f2</a> &raquo;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">3. Preprocessing data</a><ul>
<li><a class="reference internal" href="#twitter">3.1. Twitter</a><ul>
<li><a class="reference internal" href="#overview">3.1.1. Overview</a></li>
<li><a class="reference internal" href="#file-organization">3.1.2. File organization</a></li>
<li><a class="reference internal" href="#file-formats">3.1.3. File formats</a><ul>
<li><a class="reference internal" href="#raw-json-tweets">3.1.3.1. Raw JSON tweets</a></li>
<li><a class="reference internal" href="#tsv-files">3.1.3.2. TSV files</a></li>
<li><a class="reference internal" href="#preprocessing-metadata-file">3.1.3.3. Preprocessing metadata file</a></li>
<li><a class="reference internal" href="#geo-located-tweets">3.1.3.4. Geo-located tweets</a></li>
<li><a class="reference internal" href="#alternatives-that-were-considered-and-rejected">3.1.3.5. Alternatives that were considered and rejected</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#wikimedia-pageview-logs">3.2. Wikimedia pageview logs</a><ul>
<li><a class="reference internal" href="#id2">3.2.1. Overview</a></li>
<li><a class="reference internal" href="#id3">3.2.2. File organization</a></li>
<li><a class="reference internal" href="#id5">3.2.3. File formats</a><ul>
<li><a class="reference internal" href="#pagecount-files">3.2.3.1. Pagecount files</a></li>
<li><a class="reference internal" href="#metadata-file">3.2.3.2. Metadata file</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="collecting.html"
                        title="previous chapter">2. Collecting data</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="map_reduce.html"
                        title="next chapter">4. Map-Reduce with QUACreduce</a></p>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="preprocessing-data">
<h1>3. Preprocessing data<a class="headerlink" href="#preprocessing-data" title="Permalink to this headline">¶</a></h1>
<p>Typically, raw data direct from collection is not to useful. QUAC implements a
preprocessing step to translate it into more pleasant formats as well as do
some preliminary analysis. This section describes the steps to do that.</p>
<p><em>All times and dates are in UTC except as otherwise noted.</em></p>
<div class="section" id="twitter">
<h2>3.1. Twitter<a class="headerlink" href="#twitter" title="Permalink to this headline">¶</a></h2>
<div class="section" id="overview">
<h3>3.1.1. Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h3>
<p>QUAC&#8217;s Twitter pipeline has three basic steps:</p>
<ol class="arabic simple">
<li>Collect tweets using the streaming API. (<tt class="docutils literal"><span class="pre">collect</span></tt> script.)</li>
<li>Convert the tweets from the raw JSON, de-duplicate and clean them up, and
produce nicely organized and ordered TSV files. (<tt class="docutils literal"><span class="pre">parse.mk</span></tt> makefile.)</li>
<li>Geo-locate tweets that do not contain a geotag. (<tt class="docutils literal"><span class="pre">geo.mk</span></tt> makefile.) (But
see issue <a class="reference external" href="https://github.com/reidpr/quac/issues/15">#15</a>.)</li>
</ol>
</div>
<div class="section" id="file-organization">
<h3>3.1.2. File organization<a class="headerlink" href="#file-organization" title="Permalink to this headline">¶</a></h3>
<p>A fully populated data directory looks something like this:</p>
<ul class="simple">
<li><tt class="samp docutils literal"><span class="pre">raw/</span></tt> &#8212; Raw JSON tweets fresh from the Streaming API.<ul>
<li><tt class="samp docutils literal"><span class="pre">2012-04/</span></tt> &#8212; Tweets from <tt class="docutils literal"><span class="pre">collect</span></tt>. Each month gets its own
directory.<ul>
<li><tt class="samp docutils literal"><span class="pre">20120401_003115</span><em><span class="pre">.json.gz</span></em></tt> &#8212; Sequence of JSON tweet objects.</li>
<li><tt class="samp docutils literal"><span class="pre">20120401_003115</span><em><span class="pre">.stats</span></em></tt> &#8212; Text file containing a few statistics
about the above.</li>
<li><tt class="samp docutils literal"><span class="pre">20120401_003115</span><em><span class="pre">.2012-03-31.raw.tsv</span></em></tt> &#8212; Direct TSV translation
of tweets from March 31 contained in the JSON file above (i.e., still
contains duplicates and is in arbitrary order).</li>
<li><tt class="samp docutils literal"><span class="pre">20120401_003115</span><em><span class="pre">.2013-04-01.raw.tsv</span></em></tt> &#8212; A given JSON file can
have tweets from multiple days, so it may produce more than one
<tt class="docutils literal"><span class="pre">.raw.tsv</span></tt> file.</li>
<li><tt class="samp docutils literal"><span class="pre">20120401_003115</span><em><span class="pre">.d</span></em></tt> &#8211; Makefile listing dependencies related to
the above JSON and <tt class="docutils literal"><span class="pre">.raw.tsv</span></tt> files.</li>
<li>... (lots more of the above)</li>
</ul>
</li>
<li><tt class="samp docutils literal"><span class="pre">legacy/</span></tt> &#8212; Subdirectories don&#8217;t have to be named after dates.
(Perhaps you have some existing Twitter data that were not collected with
QUAC.)</li>
<li>... (more subdirs)</li>
</ul>
</li>
<li><tt class="samp docutils literal"><span class="pre">pre/</span></tt><ul>
<li><tt class="samp docutils literal"><span class="pre">2012-03-31</span><em><span class="pre">.all.tsv</span></em></tt> &#8212; Processed tweets from March 31 from all
raw JSON files. No duplicates and in ascending order by tweet ID.</li>
<li><tt class="samp docutils literal"><span class="pre">2012-03-31</span><em><span class="pre">.geo.tsv</span></em></tt> &#8212; Subset of the above that contain a
geotag.</li>
<li>... (two <tt class="docutils literal"><span class="pre">.tsv</span></tt> per day in the data)</li>
<li><tt class="samp docutils literal"><span class="pre">metadata</span></tt> &#8212; Python pickle file summarizing metadata for the above
files.</li>
</ul>
</li>
<li><tt class="samp docutils literal"><span class="pre">geo/</span></tt> &#8212; <cite>FIXME</cite></li>
</ul>
<p>In addition to the above, you will find <tt class="docutils literal"><span class="pre">.log</span></tt> files scattered about. These
are simply log files produced during processing.</p>
</div>
<div class="section" id="file-formats">
<h3>3.1.3. File formats<a class="headerlink" href="#file-formats" title="Permalink to this headline">¶</a></h3>
<div class="section" id="raw-json-tweets">
<h4>3.1.3.1. Raw JSON tweets<a class="headerlink" href="#raw-json-tweets" title="Permalink to this headline">¶</a></h4>
<p>Each raw tweet file (<tt class="docutils literal"><span class="pre">.json.gz</span></tt>) is a gzipped sequence of JSON-encoded
tweets in the <a class="reference external" href="https://dev.twitter.com/docs/platform-objects">format documented by Twitter</a>, separated by newlines
(i.e., a file cannot be parsed as a single JSON object). Non-tweet objects do
appear; these are also JSON encoded. Newlines do not appear within encoded
tweets, so they can safely be used as a separator. Files are named with a
timestamp of when collection started for that file (time zone is <em>local</em>, a
historical artifact which will not be fixed &#8211; be careful!) and placed in a
1-level hierarchy by year and month. The collection process caps the number of
tweets in each raw file to a &#8220;reasonable&#8221; number that changes occasionally.</p>
<p>Along with each <tt class="docutils literal"><span class="pre">.json.gz</span></tt> is a <tt class="docutils literal"><span class="pre">.stats</span></tt> file. This contains a few
statistics about the raw file&#8217;s data, though its key purpose is simply to mark
that the collector closed the tweet file in an orderly way. Bare <tt class="docutils literal"><span class="pre">.json.gz</span></tt>
files may be still in progress, broken, etc. and should be read with caution.
Tweets are Unicode and indeed contain high characters, so care must be taken
in handling character encodings.</p>
<p><cite>collect</cite> saves the raw bytes of each tweet it receives from the Twitter
Streaming API, without any parsing or encoding/decoding. There are a few
quirks of this stream. (I am pretty sure, but not 100% sure, that these are
all real, and not quirks of Python &#8211; they&#8217;re consistent between <cite>curl</cite>,
Firefox, and my Python code.) These quirks do not appear to affect the
parsability of the JSON.</p>
<ul>
<li><p class="first">While the encoding of the output is ostensibly UTF-8, it appears that high
characters are escaped with the &#8220;uXXXX&#8221; notation. For example:</p>
<div class="highlight-python"><pre>"text":"\u2606\u2606\u2606\u2606\u2606#Cruzeiro"</pre>
</div>
</li>
<li><p class="first">Some text has excessive escaping. For example, forward slashes do not need
to be escaped, but they are anyway:</p>
<div class="highlight-python"><pre>"source":"\u003Ca href=\"http:\/\/blackberry.com\/twitter"</pre>
</div>
</li>
</ul>
</div>
<div class="section" id="tsv-files">
<h4>3.1.3.2. TSV files<a class="headerlink" href="#tsv-files" title="Permalink to this headline">¶</a></h4>
<p>The raw tweet files are not so convenient to work with: JSON parsing is slow,
and tweets can be duplicated and out of order (including between files, which
makes parallelization difficult). Therefore, we pre-process the JSON into a
TSV format which addresses these issues. The final product is a pair of TSV
files for each day:</p>
<ul class="simple">
<li><tt class="samp docutils literal"><span class="pre">YYYY-DD-MM.</span><em><span class="pre">all</span></em><span class="pre">.tsv</span></tt> &#8212; For each day, we build one
tab-separated-values (TSV) file containing tweets created on that day, in
ascending ID order. There is no header line, no quoting, and no
within-record newlines or tabs (these are stripped before storing the
tweets). There is some other cleaup that goes on as well; consult the source
code for this. The encoding is UTF-8. The files contain the following
possibly-empty fields, in this order (note that field names generally
correspond to those in the JSON; refer to the Twitter docs):<ol class="arabic">
<li><em>id</em>: Tweet ID from Twitter (64-bit integer)</li>
<li><em>created_at</em>: When the tweet was created, in <a class="reference external" href="http://en.wikipedia.org/wiki/ISO_8601">ISO 8601 format</a>.</li>
<li><em>text</em>: The actual &#8220;message&#8221;; free text</li>
<li><em>user_screen_name</em>: free text with some restrictions</li>
<li><em>user_description</em>: free text</li>
<li><em>user_lang</em>: <a class="reference external" href="http://en.wikipedia.org/wiki/ISO_639-1">ISO 639-1</a>
language code set by user. Note that this is a fairly unreliable means of
determining the language of <tt class="docutils literal"><span class="pre">text</span></tt>. <cite>FIXME: take advantage of new
lang tweet attribute when it comes out.</cite></li>
<li><em>user_location</em>: free text</li>
<li><em>user_time_zone</em>: self-selected from a few dozen options</li>
<li><em>location_lon</em>: longitude of geotag (WGS84)</li>
<li><em>location_lat</em>: latitude of geotag</li>
<li><em>location_src</em>: code indicating source of geotag; one of:<ul>
<li><tt class="docutils literal"><span class="pre">co</span></tt>: <tt class="docutils literal"><span class="pre">coordinates</span></tt> attribute (GeoJSON)</li>
<li><tt class="docutils literal"><span class="pre">ge</span></tt>: <tt class="docutils literal"><span class="pre">geo</span></tt> attribute (an older form of official geotag) <cite>FIXME</cite></li>
<li><tt class="docutils literal"><span class="pre">lo</span></tt>: coordinates appearing in user <tt class="docutils literal"><span class="pre">location</span></tt> field <cite>FIXME</cite></li>
<li>... <cite>FIXME</cite></li>
</ul>
</li>
</ol>
</li>
<li><tt class="samp docutils literal"><span class="pre">YYYY-DD-MM.</span><em><span class="pre">geo</span></em><span class="pre">.tsv</span></tt> &#8212; The subset of the above which have a
geotag.</li>
</ul>
<p>There are also intermediate TSV files (<tt class="docutils literal"><span class="pre">.raw.tsv</span></tt>) which are in the above
format but have not yet had de-duplication and re-ordering. Downstream
applications should ignore them.</p>
<p><cite>FIXME</cite>:</p>
<ul class="simple">
<li>Try gzipping the TSV files. Some quick and dirty tests suggest that
processing time (with <tt class="docutils literal"><span class="pre">gzip</span> <span class="pre">-1</span></tt>) will roughly double and file sizes will
roughly halve.</li>
</ul>
</div>
<div class="section" id="preprocessing-metadata-file">
<h4>3.1.3.3. Preprocessing metadata file<a class="headerlink" href="#preprocessing-metadata-file" title="Permalink to this headline">¶</a></h4>
<p>This file is a pickled Python dictionary containing metadata about the
directory of preprocessed TSV files. It currently contains one item:</p>
<ol class="arabic simple">
<li><tt class="docutils literal"><span class="pre">days</span></tt> is a <tt class="docutils literal"><span class="pre">dict</span></tt> listing metadata for the daily TSV files above. Keys
are <tt class="docutils literal"><span class="pre">datetime.date</span></tt> instances, and values are dictionaries with the
following fields:<ul>
<li><em>count</em>: Number of tweets</li>
<li><em>count_geotag</em>: Number of geotagged tweets</li>
<li><em>min_id</em>: Minimum tweet ID in the file</li>
<li><em>max_id</em>: Maximum tweet ID in the file</li>
</ul>
</li>
</ol>
<p><em>Note: The metadata file used to contain information about the raw tweet files
as well. This proved to be not so useful, and so it hasn&#8217;t been reimplemented
in the new make-based processing scheme.</em></p>
</div>
<div class="section" id="geo-located-tweets">
<h4>3.1.3.4. Geo-located tweets<a class="headerlink" href="#geo-located-tweets" title="Permalink to this headline">¶</a></h4>
<p><cite>FIXME</cite></p>
<ul class="simple">
<li>TSV, one per day</li>
<li>Tweet ID, pickled Geo_GMM instance</li>
<li>GMM even if geotagged</li>
</ul>
</div>
<div class="section" id="alternatives-that-were-considered-and-rejected">
<h4>3.1.3.5. Alternatives that were considered and rejected<a class="headerlink" href="#alternatives-that-were-considered-and-rejected" title="Permalink to this headline">¶</a></h4>
<p>We tried the following and ultimately rejected them (for now). A key
requirement (as of 2/21/2013) is that we&#8217;d like convenient parallel access and
not to mess with setting up servers.</p>
<ul class="simple">
<li>Postgres: We tried using Postgres, which is a very nice open source RDBMS
that has great spatial support (PostGIS), but it was just too slow. Also, it
requires setting up a server and doesn&#8217;t lend itself to a distributed
approach.</li>
<li>DBM-style databases (e.g., BerkeleyDB): We need key/tuple storage, not just
key/value (unless we want to do our own pickling of Python objects into
values, which seems lame).</li>
<li>SQLite/SpatiaLite: Again, rather slow, and overkill since we need key/tuple
storage. Doesn&#8217;t support streaming or parallel access very well.</li>
<li>ZODB: This is a Python-native object database (from the Zope project). I
implemented it as far as actually storing data, but the documentation is
poor (e.g., the ZODB Book recommends a technique for subtransactions that
doesn&#8217;t work any more), the interface is a bit awkward, it produces several
files per database, and the databases are rather large (a subset of 8 fields
is nearly as large as the gzipped raw tweet files).</li>
<li>NoSQL: There are lots of very hip NoSQL databases (e.g. CouchDB, MongoDB,
etc.). However, none seem to offer both an embedded option (i.e., no server
process) and key/tuple (document- or column-oriented?) rather than simply
key/value.</li>
</ul>
</div>
</div>
</div>
<div class="section" id="wikimedia-pageview-logs">
<h2>3.2. Wikimedia pageview logs<a class="headerlink" href="#wikimedia-pageview-logs" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id2">
<h3>3.2.1. Overview<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>The pipeline for Wikimedia data (Wikipedia and related projects) is simpler.
We acquire them using the <tt class="docutils literal"><span class="pre">wp-get-access-logs</span></tt> script, and then build some
metadata with the <tt class="docutils literal"><span class="pre">wp-metadata.mk</span></tt> makefile.</p>
</div>
<div class="section" id="id3">
<h3>3.2.2. File organization<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>A fully populated data directory looks (in part) something like this:</p>
<ul class="simple">
<li><tt class="samp docutils literal"><span class="pre">raw/</span></tt> &#8212; Raw text files direct from WMF. Note that some of these
files contain breakage.<ul>
<li><tt class="samp docutils literal"><span class="pre">2012/</span></tt><ul>
<li><tt class="samp docutils literal"><span class="pre">2012-04/</span></tt> &#8212; Article access counts (&#8220;pageviews&#8221;) from April
2012. Each month gets its own subdirectory.<ul>
<li><tt class="samp docutils literal"><span class="pre">pagecounts-20120428-130001.gz</span></tt> &#8212; Number of times each URL was
served.</li>
<li><tt class="samp docutils literal"><span class="pre">projectcounts-20120428-130001</span></tt> &#8212; Total number of URLs served
from each project (e.g., Norwegian Wiktionary) for the same hour.
These files have a number of problems, so we don&#8217;t use them (see issue
<a class="reference external" href="https://github.com/reidpr/quac/issues/81">#81</a>).</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><tt class="samp docutils literal"><span class="pre">hashed/</span></tt> &#8212; Text files in an improved hierarchy. All of these files
will parse correctly.<ul>
<li><tt class="samp docutils literal"><span class="pre">185/</span></tt> &#8212; Pageviews whose filenames hashed to 185. Currently, we
use the DJB2 hash algorithm mod 256 (the modulus is configurable). QUAC
Wikimedia processing code is directory-parallel, so by doing this we can
operate with wider parallelism (there are currently 71 months in the data
set).<ul>
<li><tt class="samp docutils literal"><span class="pre">pagecounts-20120428-130001.gz</span></tt> &#8212; Symlink to the corresponding
pagecount file in the raw directory.</li>
</ul>
</li>
</ul>
</li>
<li><tt class="samp docutils literal"><span class="pre">hashed_small/</span></tt> &#8212; Subset of the above, retaining only the midnight
to 1am data for each day; this sampling strategy avoids introducing new gaps
in the data. This is for testing and yields a dataset somewhat less than 4%
the size of the full dataset.</li>
<li><tt class="samp docutils literal"><span class="pre">hashed_tiny/</span></tt> &#8212; An even tinier subset (not necessarily a subset of
the small subset). Sampling strategy varies, but the goal is about 2-3 GB of
compressed pageview data. Note that available parallelism is less than the
full dataset.</li>
<li><tt class="samp docutils literal"><span class="pre">metadata</span></tt> &#8212; The metadata file.</li>
</ul>
</div>
<div class="section" id="id5">
<h3>3.2.3. File formats<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<div class="section" id="pagecount-files">
<h4>3.2.3.1. Pagecount files<a class="headerlink" href="#pagecount-files" title="Permalink to this headline">¶</a></h4>
<p>The file format of the pagecount files is <a class="reference external" href="http://dumps.wikimedia.org/other/pagecounts-raw/">documented by WMF</a>. There are some quirks:</p>
<ol class="arabic">
<li><p class="first">The timestamp in the filename is the <em>end</em> of the hour recorded in the
file. Often, these timestamps are a few seconds past the hour; we ignore
this.</p>
</li>
<li><p class="first">Filesystem timestamps are not reliable, especially in the older parts of
the data. That is, sometimes older files have newer timestamps, and the
interval between consecutive files can be much less than one hour
(sometimes less than a second, making them equal on many filesystems).</p>
</li>
<li><p class="first">The files are ASCII, with high bytes in article URLs percent-encoded. We do
not decode them because (a) it saves significant time and (b) there are
apparently non-UTF-8 encodings in use. (I believe the URL encoding is
selected by the browser.)</p>
<p>An artifact of (b) is that article counts can be split. For example, the
Russian article Люди_Икс
(i.e., the X-Men comic series) can be accessed at both of the following
URLs:</p>
<ul class="simple">
<li>(UTF-8) <a class="reference external" href="http://ru.wikipedia.org/wiki/%D0%9B%D1%8E%D0%B4%D0%B8_%D0%98%D0%BA%D1%81">http://ru.wikipedia.org/wiki/%D0%9B%D1%8E%D0%B4%D0%B8_%D0%98%D0%BA%D1%81</a></li>
<li>(Windows-1251) <a class="reference external" href="http://ru.wikipedia.org/wiki/%CB%FE%E4%E8_%C8%EA%F1">http://ru.wikipedia.org/wiki/%CB%FE%E4%E8_%C8%EA%F1</a></li>
</ul>
<p>Other encodings (e.g., ISO 8859-5, %BB%EE%D4%D8_%B8%DA%E1 and KOI8-R,
%EC%C0%C4%C9_%E9%CB%D3) do not work. Figuring out this mess is something
I&#8217;m not very interested in. How WMF does it, I have no idea.</p>
<p>We do, however, normalize spaces into underscores. I believe this may be
incomplete (see issue #77).</p>
</li>
<li><p class="first">There have been periods of modest <a class="reference external" href="http://dumps.wikimedia.org/other/pagecounts-ez/projectcounts//readme.txt">underreporting</a>,
with up to 20% of hits unrecorded. We assume such underreporting is random
and do not try to correct it. Because our analysis works on fraction of
total traffic rather than raw hit counts, the effect should be minimal.</p>
</li>
</ol>
</div>
<div class="section" id="metadata-file">
<h4>3.2.3.2. Metadata file<a class="headerlink" href="#metadata-file" title="Permalink to this headline">¶</a></h4>
<p>This is a pickled Python dictionary. Example content:</p>
<div class="highlight-python"><pre>{ 'badfiles': set([ ... ]),
  'projects': { 'en':   { date: { 'total': count,
                                  'hours': {  0: count,
                                              1: count,
                                              ... ,
                                             23: count }],
                          ... }
                'en.b': ... ,
                'ru':   ... ,
                ... } }</pre>
</div>
<p>That is, there are two items in the dictionary.</p>
<ol class="arabic">
<li><p class="first"><tt class="docutils literal"><span class="pre">badfiles</span></tt> is a set of paths (starting with <tt class="docutils literal"><span class="pre">raw/</span></tt>) which are the files
that had parse errors. These files are the ones excluded from <tt class="docutils literal"><span class="pre">hashed/</span></tt>.</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">projects</span></tt>, keys are project codes from the pageview files (e.g.,
<tt class="docutils literal"><span class="pre">fr.b</span></tt> for French Wikibooks). Values are themselves dictionaries, mapping
<tt class="docutils literal"><span class="pre">datetime.date</span></tt> instances to the number of hits that day. The hits
consist of a dictionary. Item <tt class="docutils literal"><span class="pre">'total'</span></tt> gives the total number of hits on
that day, while item <tt class="docutils literal"><span class="pre">'hours'</span></tt> is a dictionary mapping hours (0 to 23) to
the number of hits in that hour.</p>
<p>In both cases, the value <tt class="docutils literal"><span class="pre">0</span></tt> means no hits. A missing date or a missing
hour means no data (i.e., the <tt class="docutils literal"><span class="pre">'hours'</span></tt> dict would have only 23 entries
if one hour of data were missing on that day).</p>
</li>
</ol>
</div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="map_reduce.html" title="4. Map-Reduce with QUACreduce"
             >next</a></li>
        <li class="right" >
          <a href="collecting.html" title="2. Collecting data"
             >previous</a> |</li>
        <li><a href="index.html">QUAC documentation for commit 6b395f2</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2012-2013, Los Alamos National Security, LLC and others.
    </div>
  </body>
</html>