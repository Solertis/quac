#!/usr/bin/env python3

# Copyright Â© Los Alamos National Security, LLC, and others.

'''Update a time series dataset containing Wikipedia pageview data

Summary statistics related to performance:

  * The Wikipedia datafiles contain about 8M lines each.
  * There are 720 data files in a 30-day month.
  * Therefore, we need to process about 5.7G lines per month of data.
  * To do so in 24 hours, we need to move 65,000 lines per second.

This is well beyond the capacity of a naive read-modify-write write strategy.
Thus, we have four different write strategies depending on the update desired.

A key observation is that article hits follow a long-tail distribution. Most
articles have only a handful of hits in a month (perhaps because they are not
real articles) and can therefore be safely discarded; even if such articles
carried a useful signal, relying on them would be risky due to noise and the
opportunity for manipulation. (The threshold is configured at
wkpd.keep_threshold.) In the bulk-loading situation, which is of the most
concern for performance, these monthly hit counts are known without needing to
reference any data from the existing file, and article vectors which do not
pass the threshold can be discarded without writing them at all.

Thus, we use the following write strategies. Recall that the --prune switch
indicates an fragment file is complete (i.e., the month is closed).

  Is --prune specified?
  |   Was the data file empty when opened?
  |   |
  |   |    Strategy
  |   |    |  Write strategy
  |   |    |  |                  Prune strategy
  --- ---  -  -----------------  -----------------------------------------

  no  no   0  read-modify-write  none
  no  yes  1  write-only         none
  yes no   2  read-modify-write  eager + batch
  yes yes  3  write-only         eager

Write strategies:

  read-modify-write -- If a fragment already exists on disk, fetch it, update
     it with all available new data, and save it again. If not, create it from
     scratch as a zero vector instead of fetching.

  write-only -- Create all fragments as zero vectors from scratch, ignoring
     any existing on-disk fragments. This will cause primary key errors if any
     such fragments really do exist, so it must only be used on empty files.

Prune strategies:

  eager -- Do not write fragments below the threshold; insead, discard them.
     This means that existing saved fragments which do not meet the threshold
     will be left stale in the dataset.

  batch -- After fragment writing is done, remove all saved fragments which do
     not meet the threshold. When combined with eager, this will remove stale
     eager-pruned fragments, which are guaranteed to have a smaller total than
     the updated version which was not saved.

When is each strategy used?

  0. This is the standard daily incremental update strategy. This operates at
     several thousand articles per second, enough to process one day's worth
     of data in a few hours.

  1. This happens for the daily update the first time it is run each month.
     Again, performance is acceptable due to the small number of input data.

     This strategy is also important for bulk loading, in that it is used for
     the last, incomplete month of the bulk load.

  2. This happens for the daily update the last time it is run each month.
     Because it must process the entire file, it takes significantly longer
     than most daily updates.

  3. This is the fastest data path, used for bulk loading of full months. It
     avoids both lookup/read I/O and writing most article vectors (which, due
     to the long-tail distribution of traffic, do not pass the threshold).

Note that if subsequent days' updates overlap, this script will fail, which
may risk corruption on some filesystems. This is most likely to happen on the
first update of the month (i.e., the update after strategy 2).'''

import datetime
import heapq
import itertools
import os
import sys
import time

import apsw
import numpy as np
import pytz

import quacpath
import time_
import timeseries
import testable
import u
import wikimedia

c = u.c
l = u.l


### Setup ###

ap = u.ArgumentParser(description=__doc__)
gr = ap.default_group
gr.add_argument('--warn-duplicates',
                action='store_true',
                help='warn if article found more than once (lots of memory!)')
gr.add_argument('--limit',
                metavar='N',
                type=int,
                default=sys.maxsize,
                help='stop processing after saving this many time series')
gr.add_argument('--prune',
                action='store_true',
                help='prune and compact the dataset')
gr.add_argument('--stats',
                metavar='N',
                type=int,
                help='print statistics every N writes (implies --verbose)')
gr.add_argument('outfile',
                metavar='OUTFILE',
                help='time series dataset to create or update')
gr.add_argument('pv_files',
                metavar='PAGECOUNT',
                nargs='+',
                help='pagecount files to add')

class Out_of_Order_Error(Exception):
   def __init__(self, line_num):
      super().__init__('out of order at line %d' % line_num)

### Script ###

def main():
   l.info('starting')
   args.keep_threshold = int(c['wkpd']['keep_threshold'])
   os.environ['SQLITE_TMPDIR'] = args.outfile
   (month, pv_files) = pv_files_validated(args.pv_files)
   ds = timeseries.Dataset(args.outfile, 'H', int(c['wkpd']['hashmod']),
                           writeable=True)
   num_hours = time_.hours_in_month(month)
   fg = ds.open_timestamp(month, num_hours)
   outfile_mtime = fg.mtime
   l.info('opened %s/%s length %d hours' % (args.outfile, fg.tag, fg.length))
   args.file_empty_p = fg.empty_p()
   args.eager_prune_p = args.prune
   fg.begin()
   files_process(fg, pv_files)
   fg.commit()
   if (args.prune):
      if (not args.file_empty_p):
         # Strategy 2: batch prune
         start = time.time()
         fg.prune(args.keep_threshold)
         l.info('pruned to %d in %s' % (args.keep_threshold,
                                        u.fmt_seconds(time.time() - start)))
      # Strategies 2 and 3. Vacuuming apparently helps even under Strategy 3,
      # where I assumed it wouldn't, because we insert in sequential order
      # with no deletions. However, some very informal tests suggest a space
      # savings of 10% and single-article query performance improvement of 2x,
      # at the cost of several hours of vacuuming.
      start = time.time()
      fg.vacuum()
      l.info('vacuumed in %s' % u.fmt_seconds(time.time() - start))
   ds.close()
   fg.mtime = mtime_max(outfile_mtime, *pv_files)
   l.info('done')

def file_read(file_, project_re):
   try:
      ts = wikimedia.timestamp_parse(file_)
      hour_month_offset = time_.hour_month_offset(ts)
      fp = u.zcat(file_, r"zcat '%%s' | egrep '^%s [-A-Za-z0-9_~!*();@,./%%%%]+ [0-9]+ [0-9]+$'" % project_re)
      badline_ct = 0
      prev_line = b''
      for (i, line) in enumerate(fp):
         try:
            if (line < prev_line):
               raise Out_of_Order_Error(i+1)  # warning: not file line number
            (proj, url, count, _) = line.split(b' ')
            prev_line = line
            yield (proj.decode('ascii'), url.decode('ascii'),
                   hour_month_offset, int(count))
         except ValueError as x:
            # Ignore lines that don't parse. Some files have thousands of
            # these (pagecounts-20130201-010000.gz), and many files have at
            # least one (all of February 2013). However, decoding errors
            # should never happen, because we filter out any potential
            # problems with egrep.
            if (isinstance(x, UnicodeDecodeError)):
               raise x
            badline_ct += 1
      if (badline_ct > 0):
         l.warning('%s: %d lines with parse errors skipped'
                   % (file_, badline_ct))
   except (EOFError, IOError, Out_of_Order_Error) as x:
      l.warning('%s: read error, skipping rest: %s' % (file_, str(x)))

def files_process(fg, files):
   def fetch_or_create(proj, dtype, fill=None):
      if (args.file_empty_p):
         return fg.create(proj, dtype=dtype, fill=fill)
      else:
         return fg.fetch_or_create(proj, dtype=dtype, fill=fill)
   keep_threshold = args.keep_threshold if args.eager_prune_p else -1
   l.info('write strategy %d (eager prune=%d, empty=%d), keep threshold=%d'
          % (args.eager_prune_p * 2 + args.file_empty_p, args.eager_prune_p,
             args.file_empty_p, keep_threshold))
   line_ct = 0
   url_total_ct = 0
   url_write_ct = 0
   start = time.time()
   proj_totals = None
   proj_last = None
   articles_seen = set()
   stats_printed = True
   for ((proj, url), gr) in itertools.groupby(files_read(files),
                                              key=lambda i: i[:2]):
      url_total_ct += 1
      if (args.warn_duplicates):
         if ((proj, url) in articles_seen):
            l.warn('duplicate article found: %s+%s' % (proj, url))
         articles_seen.add((proj, url))
      if (proj != proj_last):
         if (proj_last is not None):
            try:
               proj_totals.save(keep_threshold)
            except apsw.ConstraintError:
               u.abort('duplicate project, cannot save: %s' % proj_last)
         proj_last = proj
         proj_totals = fetch_or_create(proj, np.float64, fill=np.nan)
      url_v = fetch_or_create('%s+%s' % (proj, url), np.float32)
      for (_, _, hour_month_offset, count) in gr:
         line_ct += 1
         url_v.data[hour_month_offset] = count
         if (np.isnan(proj_totals.data[hour_month_offset])):
            proj_totals.data[hour_month_offset] = count
         else:
            proj_totals.data[hour_month_offset] += count
      if (url_v.save(keep_threshold)):
         url_write_ct += 1
         stats_printed = False # avoid multiple stats after non-write iterations
      if (args.stats and url_write_ct % args.stats == 0 and not stats_printed):
         l.debug('... statistics after %d writes ...' % url_write_ct)
         l.debug('current article: %s' % url_v.name)
         u.memory_use_log()
         l.debug('SQLite malloc: %s now, %s max'
                 % (u.fmt_bytes(apsw.memoryused()),
                    u.fmt_bytes(apsw.memoryhighwater())))
         l.debug('SQLite pagecache pages: %d now, %s max'
                 % apsw.status(apsw.SQLITE_STATUS_PAGECACHE_USED))
         stats_printed = True
      if (url_write_ct >= args.limit):
         break
   if (proj_last is not None):
      proj_totals.save(keep_threshold)
   time_used = time.time() - start
   l.info('read %s lines in %s (%d lines/s)'
          % (line_ct, u.fmt_seconds(time_used), (line_ct) / time_used))
   try:
      l.info('%d of %d URLs saved (%.1f%%, %d total/s)'
             % (url_write_ct, url_total_ct, 100 * url_write_ct / url_total_ct,
                url_total_ct / time_used))
   except ZeroDivisionError:
      pass

def files_read(pv_files):
   pipes = list()
   # While there is a period near the beginning of the data which appear to
   # contain fully sorted files, I don't trust this tremendously, so I only
   # test against the later transition back to fully-sorted. (See the docs.)
   for f in pv_files:
      ts = wikimedia.timestamp_parse(f)
      if (ts <= time_.iso8601_parse('2015-01-15')):
         # dot and non-dot sorted separately; use two pipes
         pipes.append(file_read(f, r'[a-z]+'))
         pipes.append(file_read(f, r'[a-z]+\.[a-z]+'))
      else:
         # fully sorted; need only one pipe
         pipes.append(file_read(f, r'[a-z.]+'))
   # We pursued an external "sort -m" based merge, but this turns out to
   # perform just as well and is simpler.
   return heapq.merge(*pipes)

def mtime_max(*files):
   '''Compute a "maximum" mtime which is two microseconds after the last
      time found in the arguments, which can be float timestamps or strings,
      the latter assumed to be filenames from which mtime is gathered.

      The two microseconds are because some Linux filesystems (e.g., ext4)
      store timestamps to nanosecond resolution, but Python can only set to
      microsecond resolution. For example, copying the mtime 2013-11-12
      14:46:35.567961144 will result in 2013-11-12 14:46:35.567961000, which
      is earlier; therefore, unnecessary files may be processed. Adding a
      small interval guarantees that the metadata file is the same or newer.
      We use two microseconds rather than one because I saw (but couldn't
      reproduce) an example of make not believing a one-microsecond file to be
      newer, which I think is attributable to a rounding error of some kind.

      This assumes that the earliest file processed in the *next* run is at
      least two microseconds newer than the latest file in this run. This
      assumption is violated in some of the older data, I believe that which
      has been moved around at some point, so use caution.'''
   def mtime(f):
      if (isinstance(f, str)):
         return u.mtime(f)
      else:
         return f
   return max(mtime(f) for f in files) + 2e-6

def pv_files_validated(pv_files):
   # Check month consistency in reverse order because the most common cause of
   # the same-month warning is a glob expression like "pagecounts-201210*",
   # where the outlier is the first item in the list. In this case, we want
   # one warning/skip rather than hundreds.
   ts_wanted = None
   month_wanted = None
   pv_files_checked = list()
   for file_ in reversed(pv_files):
      if (not os.path.isfile(file_)):
         u.abort('%s is not a file' % file_)
      try:
         ts = wikimedia.timestamp_parse(file_)
         month = ts.strftime('%Y%m')
      except ValueError:
         u.abort('%s is not named like a Wikimedia pagecount file' % file_)
      if (month_wanted is not None and month != month_wanted):
         l.warning('not from month %s, skipping: %s' % (month_wanted, file_))
      else:
         pv_files_checked.append(file_)
         ts_wanted = ts
         month_wanted = month
   month_dt = datetime.datetime(ts_wanted.year, ts_wanted.month, 1,
                                tzinfo=pytz.utc)
   return (month_dt, sorted(pv_files_checked))


### Bootstrap ###

try:
   args = u.parse_args(ap)
   if (args.stats):
      u.verbose = True
   u.configure(args.config)
   u.logging_init('wptsu')
   if (__name__ == '__main__'):
      main()
except testable.Unittests_Only_Exception:
   testable.register()
